# 第一章 简介

**作者：吴恩达教授**

亲爱的开发人员们，欢迎加入本次深入探索ChatGPT提示词工程（Prompt Engineering）的旅程。本课程由我与Isa Fulford教授共同主持，Isa作为OpenAI技术团队的核心成员，在ChatGPT检索插件开发、LLM（Large Language Model，大语言模型）技术在产品中的创新应用以及Prompt使用教程的编写等方面均有着丰富的实战经验。

尽管网络上关于提示词（Prompt）的资料浩如烟海，但多数内容主要聚焦于ChatGPT的Web界面，侧重于执行一次性任务。然而，对于我们这些致力于软件开发的同仁们来说，LLM的真正魅力在于通过API调用，快速构建出功能强大的软件应用程序。遗憾的是，这一领域尚未得到足够的重视。在DeepLearning.AI的姊妹公司AI Fund，我们的团队一直在与众多初创公司紧密合作，将LLM技术应用于各种实际场景中。看到LLM API能够帮助开发人员迅速构建出各种应用，我们感到无比兴奋。

在本课程中，我们将分享一系列实用的技巧，帮助大家充分挖掘LLM的潜力，并提供丰富的应用案例和最佳实践。我们将深入探讨用于软件开发的Prompt最佳实践，介绍几个常见的使用场景，包括概括、推断、转换与扩展等，并重点讲解如何利用LLM构建chatbot（聊天机器人），激发大家的创新思维，开拓更多应用领域。

## LLM技术分类与特点

随着LLM技术的不断发展，我们可以将其大致分为基础LLM和指令微调（Instruction Tuned）LLM两类。这两类模型在功能和应用场景上各有特点。

### 基础LLM

基础LLM主要基于文本训练数据，通过预测下一个单词的能力来构建模型。这类模型在互联网和其他来源的大量数据上进行训练，以预测最可能的后续单词。然而，由于训练数据的多样性，基础LLM在某些情况下可能会产生不符合预期的输出。

### 指令微调的LLM

指令微调的LLM则更加注重遵循人类指令。通过进一步训练与微调，这类模型能够更好地理解并执行人类输入的指令。因此，当您询问“法国的首都是什么？”时，指令微调的LLM更有可能给出准确的答案“法国的首都是巴黎”。此外，由于指令微调的LLM已经被训练成有益、诚实、无害的，因此它们更不太可能产生有问题的文本输出。

鉴于指令微调的LLM在实际应用中的显著优势，本课程将重点介绍针对这类模型的最佳实践。我们强烈建议您在大多数使用场景中优先考虑使用指令微调的LLM。

## 使用指令微调的LLM的最佳实践

在使用指令微调的LLM时，您可以将其视为一个聪明但对您任务细节不完全了解的人。因此，当LLM无法正确响应时，很可能是因为您提供的指令不够清晰明确。

### 提高LLM响应质量的关键方法

为了提高LLM的响应质量，您可以尝试以下方法：

1. **明确说明任务细节**：在编写提示词时，务必明确说明您希望LLM关注的具体方面（如科学工作、个人生活等），以及您期望的语调（如专业记者写作风格或朋友间的随笔风格等）。这将有助于LLM更好地理解您的需求，并生成更符合您期望的响应。

2. **指定参考文本**：您可以指定LLM应该参考哪些文本片段来生成回答。这可以是您自己的文本，也可以是其他可靠的来源。通过指定参考文本，您可以为LLM提供更多的上下文信息，从而进一步提高其回答的质量和准确性。

## 后续章节预览

在接下来的章节中，我们将详细介绍提示词创建的两个关键原则：清晰明确和给LLM时间去思考。通过遵循这些原则，您将能够更有效地利用LLM技术，构建出更加智能、实用的应用程序。让我们携手共进，开启这场关于ChatGPT提示词工程的探索之旅吧！